{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Bootstrap Aggregating\n",
    "* Train each classifier on a subset the WSIs.Not true bagging because no replacement. Can't be true, because there isn't a fixed dateset.\n",
    "* Try training each classifier on half, quarter of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/rene/Data/camelyon/src\n",
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "SEED = 101\n",
    "np.random.seed(SEED)\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Add the src directory for functions\n",
    "src_dir = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'src')\n",
    "print(src_dir)\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "# import my functions:\n",
    "from WSI_pytorch_utils import*\n",
    "\n",
    "fast_ai_dir = '/media/rene/Data/fastai/'\n",
    "sys.path.append(fast_ai_dir)\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try training with normal batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_full_'\n",
    "batch_size = 64\n",
    "data_loc = '/media/rene/Data/camelyon_out/tiles_224_100t'\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_loc, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'valid']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=6)\n",
    "              for x in ['train', 'valid']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_sizes)\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# all parameters are being optimized\n",
    "#     optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_ft = optim.Adam(model_ft.parameters())\n",
    "\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                   num_epochs=epochs)\n",
    "\n",
    "torch.save(model_ft.state_dict(), save_path+'normal_batch_gen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accu(model, test_loader, ttv='train'):\n",
    "    model.eval()\n",
    "    correct_count = 0.0\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        img, labels = data\n",
    "        x = Variable(img, volatile=True)\n",
    "        y = Variable(labels, volatile=True)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            x=torch.squeeze(x)\n",
    "        outs = model(x)\n",
    "        _, pred = torch.max(outs, -1)\n",
    "        correct_count += (pred == y).sum().data[0]\n",
    "        del x, y, pred, outs\n",
    "    print('Accu: {}'.format(correct_count/dataset_sizes[ttv]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accu: 0.9285234093637454\n"
     ]
    }
   ],
   "source": [
    "model_ft= models.resnet50(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.cuda()\n",
    "\n",
    "model_ft.load_state_dict(torch.load('/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_full_normal_batch_gen.h5'))\n",
    "model_ft.eval()\n",
    "model_ft.train(False)\n",
    "\n",
    "test_accu(model_ft, test_loader = dataloaders['train'], ttv='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accu: 0.922814371257485\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.cuda()\n",
    "\n",
    "model_ft.load_state_dict(torch.load('/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_full_normal_batch_gen.h5'))\n",
    "model_ft.eval()\n",
    "model_ft.train(False)\n",
    "\n",
    "test_accu(model_ft, test_loader = dataloaders['valid'], ttv='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online batches \n",
    "* Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = '/media/rene/Data/CAMELYON16/TrainingData'\n",
    "batch_size = 80\n",
    "dataset_sizes = {}\n",
    "dataset_sizes['train'] = 100000\n",
    "dataset_sizes['valid'] = 10000\n",
    "\n",
    "ttv_split = np.load('/media/rene/Data/camelyon/other/ttv_split.p')\n",
    "normal_valid = ttv_split['normal_vaild_idx']\n",
    "tumor_valid = ttv_split['tumor_vaild_idx']\n",
    "normal_train = list(range(1, 161))\n",
    "normal_train = [num for num in normal_train if num not in normal_valid]\n",
    "tumor_train = list(range(1, 111))\n",
    "tumor_train = [num for num in tumor_train if num not in tumor_valid]\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "train_dataset = WSIDataset(data_loc, normal_train, tumor_train, batch_size, length=dataset_sizes['train'], transforms=data_transforms['train'])\n",
    "valid_dataset = WSIDataset(data_loc, normal_valid, tumor_valid, batch_size, length=dataset_sizes['valid'], transforms=data_transforms['valid'])\n",
    "\n",
    "dataloaders ={}\n",
    "# batch size of 1 because of the weird get item\n",
    "dataloaders['train'] = DataLoader(train_dataset,  batch_size=1, num_workers=6, shuffle=False)\n",
    "dataloaders['valid'] = DataLoader(valid_dataset, batch_size=1, num_workers=6, shuffle=False)\n",
    "\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_full_'\n",
    "\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# all parameters are being optimized\n",
    "#     optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_ft = optim.Adam(model_ft.parameters())\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                   num_epochs=epochs)\n",
    "\n",
    "torch.save(model_ft.state_dict(), save_path+'online_batch_gen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accu: 0.74497\n",
      "Accu: 0.7217\n"
     ]
    }
   ],
   "source": [
    "model_ft= models.resnet50(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.cuda()\n",
    "\n",
    "model_ft.load_state_dict(torch.load('/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_full_online_batch_gen.h5'))\n",
    "model_ft.eval()\n",
    "model_ft.train(False)\n",
    "\n",
    "test_accu(model_ft, test_loader = dataloaders['train'], ttv='train')\n",
    "test_accu(model_ft, test_loader = dataloaders['valid'], ttv='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try standard dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accu: 0.7147298919567827\n",
      "Accu: 0.7641916167664671\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "data_loc = '/media/rene/Data/camelyon_out/tiles_224_100t'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_loc, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'valid']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=6)\n",
    "              for x in ['train', 'valid']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "model_ft= models.resnet50(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.cuda()\n",
    "\n",
    "model_ft.load_state_dict(torch.load('/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_full_online_batch_gen.h5'))\n",
    "model_ft.eval()\n",
    "model_ft.train(False)\n",
    "\n",
    "test_accu(model_ft, test_loader = dataloaders['train'], ttv='train')\n",
    "test_accu(model_ft, test_loader = dataloaders['valid'], ttv='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line by line check of sampling locations? \n",
    "* Maybe the different masks for the normal in the two versions caused a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 20/1250 [00:22<23:01,  1.12s/it] "
     ]
    }
   ],
   "source": [
    "save_path = '/media/rene/Data/camelyon_out/online_batch/resnet50_10e_full_'\n",
    "\n",
    "for i in range(5):\n",
    "    train_dataset = WSIDataset(data_loc, normal_train, tumor_train, batch_size, length=dataset_sizes['train'], transforms=data_transforms['train'])\n",
    "    valid_dataset = WSIDataset(data_loc, normal_valid, tumor_valid, batch_size, length=dataset_sizes['valid'], transforms=data_transforms['valid'])\n",
    "\n",
    "    dataloaders ={}\n",
    "    # batch size of 1 because of the weird get item\n",
    "    dataloaders['train'] = DataLoader(train_dataset,  batch_size=1, num_workers=6, shuffle=False)\n",
    "    dataloaders['valid'] = DataLoader(valid_dataset, batch_size=1, num_workers=6, shuffle=False)\n",
    "\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # all parameters are being optimized\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters())\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n",
    "\n",
    "    model_ft = train_model(model_ft, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=epochs)\n",
    "    \n",
    "    torch.save(model_ft.state_dict(), save_path+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_half_'\n",
    "\n",
    "for i in range(5):\n",
    "    normal_train_subset = random.sample(normal_train, int(len(normal_train)/2))\n",
    "\n",
    "    train_dataset = WSIDataset(data_loc, normal_train_subset, tumor_train, batch_size, length=dataset_sizes['train'], transforms=data_transforms['train'])\n",
    "    valid_dataset = WSIDataset(data_loc, normal_valid, tumor_valid, batch_size, length=dataset_sizes['valid'], transforms=data_transforms['valid'])\n",
    "\n",
    "    dataloaders ={}\n",
    "    dataloaders['train'] = DataLoader(train_dataset,  batch_size=1, num_workers=6, shuffle=False)\n",
    "    dataloaders['valid'] = DataLoader(train_dataset, batch_size=1, num_workers=6, shuffle=False)\n",
    "\n",
    "\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=4, gamma=0.1)\n",
    "\n",
    "    model_ft = train_model(model_ft, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=epochs)\n",
    "    \n",
    "    torch.save(model_ft.state_dict(), save_path+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_quarter_'\n",
    "\n",
    "for i in range(5):\n",
    "    normal_train_subset = random.sample(normal_train, int(len(normal_train)/4))\n",
    "\n",
    "    train_dataset = WSIDataset(data_loc, normal_train_subset, tumor_train, batch_size, length=dataset_sizes['train'], transforms=data_transforms['train'])\n",
    "    valid_dataset = WSIDataset(data_loc, normal_valid, tumor_valid, batch_size, length=dataset_sizes['valid'], transforms=data_transforms['valid'])\n",
    "\n",
    "    dataloaders ={}\n",
    "    dataloaders['train'] = DataLoader(train_dataset,  batch_size=1, num_workers=6, shuffle=False)\n",
    "    dataloaders['valid'] = DataLoader(train_dataset, batch_size=1, num_workers=6, shuffle=False)\n",
    "\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=4, gamma=0.1)\n",
    "\n",
    "    model_ft = train_model(model_ft, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=epochs)\n",
    "    \n",
    "    torch.save(model_ft.state_dict(), save_path+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "data_loc = '/media/rene/Data/camelyon_out/tiles_224_100t'\n",
    "batch_size = 32\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_loc, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'valid']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'valid']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
    "\n",
    "###############\n",
    "\n",
    "# model_ft= models.resnet50(pretrained=True)\n",
    "# num_ftrs = model_ft.fc.in_features\n",
    "# model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "# model_ft = model_ft.cuda()\n",
    "\n",
    "# model_ft.load_state_dict(torch.load('/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_half_1.h5'))\n",
    "# model_ft.eval()\n",
    "\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# # test_accu(model=model_ft, test_loader=dataloaders['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accu: 0.804251497005988\n"
     ]
    }
   ],
   "source": [
    "model_ft= models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.cuda()\n",
    "\n",
    "model_ft.load_state_dict(torch.load('/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_full_0.h5'))\n",
    "model_ft.eval()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "test_accu(model=model_ft, test_loader=dataloaders['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accu: 0.751317365269461\n"
     ]
    }
   ],
   "source": [
    "model_ft= models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.cuda()\n",
    "\n",
    "model_ft.load_state_dict(torch.load('/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_full_1.h5'))\n",
    "model_ft.eval()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "test_accu(model=model_ft, test_loader=dataloaders['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accu: 0.7355688622754492\n"
     ]
    }
   ],
   "source": [
    "model_ft= models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.cuda()\n",
    "\n",
    "model_ft.load_state_dict(torch.load('/media/rene/Data/camelyon_out/inline_batch_random/resnet50_10e_full_2.h5'))\n",
    "model_ft.eval()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "test_accu(model=model_ft, test_loader=dataloaders['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.eval()\n",
    "x,y=next(iter(dataloaders['valid']))\n",
    "y = torch.cat(y)\n",
    "x=torch.squeeze(x)\n",
    "x = Variable(x).cuda()\n",
    "y = Variable(y).cuda()\n",
    "\n",
    "preds = model_ft(x)\n",
    "\n",
    "del x, y, preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
